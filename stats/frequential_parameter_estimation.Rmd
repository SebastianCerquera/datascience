---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import pandas as pd
import seaborn as sns
```

```{python}
import numpy as np
from functools import partial

import matplotlib.pyplot as plt
```

<!-- #region -->
# Estimación frequentista de parametros

Estoy siguiendo este curso para aprender acerca de inferencia estadistica, Este video toca el tema desde un punto de vista frecuentista.

- https://www.youtube.com/watch?v=4UJc0S8APm4

Dado un parametro $\theta$ y se produce una variable aleatoria, para la cual nuestro estimador genera otra variable aleatoria $\Theta$ que es nuestro valor estimado. El estimador funciona tanto como con valores escalares como con vectores.

<img  src="images/estimator.png"/>

El objetivo es construir un estimador con error ($\Theta$ - $\theta$) tan pequeno como sea posible.


<img  src="images/estimator-caracteristics.png"/>

Un buen estimador cumple con las siguientes caracteristicas, el valor de $\theta$ es desconocido, 

- No tiene sesgos: El valor esperado del estimador $E[\Theta]$ se acerca a $\theta$
- Es consistente: $\Theta_n$ --> $\theta$ 
- Tiene un mean square error pequeno: 



El describe 2 metodos para construir un estimador de un parametro:

- Maximun likehood
- Sample distribution (CLT theorem)
<!-- #endregion -->

### Preguntas ???

- Que tan pequeño debe ser el MSE? Que es pequeño en este contexto?

```{python}
def check_bias(distribution_generator, estimator, N=1000):
    results = []
    for i in range(N):
        results.append(estimator(distribution_generator()))
    return np.mean(results)

def compute_samples_table(distribution_generator, estimator):
    results = []
    for i in [1, 10, 100, 1000, 10000]:
        X = distribution_generator(i)
        mu = estimator(X)
        df = pd.DataFrame({'values': X})
        df.loc[:, "estimate"] = mu
        df.loc[:, "sample"] = i
        results.append(df)
    return pd.concat(results)

def estimators_samples_fixed_N(distribution_generator, estimator, N):
    results = []
    for i in [1, 10, 100, 1000, 10000]:
        estimators = []
        for j in range(i):
            mu = estimator(distribution_generator(N))
            estimators.append(mu)
        df = pd.DataFrame({'values': estimators})
        df.loc[:, "sample"] = i
        results.append(df)
    return pd.concat(results)

def estimators_samples_table(distribution_generator, estimator):
    results = []
    for i in [1, 10, 100, 1000, 10000]:
        df = estimators_samples_fixed_N(distribution_generator, 
                                        estimator, i)
        df.loc[:, "N"] = i
        results.append(df)
    return pd.concat(results)
```

## Maximun likehood

```{python}

```

# Sample distribution

Construir un estimador usando el teorema central del limite:

- Verificar que el valor esperado del estimador se apróxime al valor del parametro.
- Verificar que el valor del estimador mejora y consistente conforme incrementa el número de muestras.
- Verificar el mean squre error, debería ser más pequeno entre mas datos por la ley de los grandes números.

### Sample mean distribution

El estimador se puede construir con el promedio de los datos de la muestra: $\dfrac{\sum_{n=1}^{N} x_i} N$

```{python}
def estimate_mean(X):
    return np.mean(X)
```

### Normal distribution

Usar el paquete de numpy para generar muestras de una distribución normal, construir el estimador para la media y verificar los resultados.

```{python}
MU = 10
SIGMA = 1

def gen_normal_sample(N, mu=MU, sigma=SIGMA):
    return np.random.normal(mu, sigma, N)
```

### Normal distribution: Sesgo

```{python}
MSG = """
El estimador no presenta es menos volatil cuando incrementa el número de muestras
"""

samples = list(range(1, 10000, 100))
expected_values = list(map(lambda e: check_bias(
    partial(gen_normal_sample, 10), estimate_mean, N=e), samples))
plt.plot(samples, expected_values, 'o')
```

```{python}
MSG = """
Hacer un box plot con la distribución de probabilidad de la muestra con estos tamanos:
1, 10, 100, 1000, 10000, 100000.

El resultado tiene sentido entre más puntos las distribución tiene menos
sesgo.
"""

df = compute_samples_table(gen_normal_sample, estimate_mean)
sns.boxplot(data=df, y="values", x="sample", showfliers=True)
```

### Normal distribution: Consistencia

```{python}
MSG = """
El estimado debería acercarcse a teta, pero no lo esta haciendo.

Algo estoy haciendo mal, el dice que el estimador tiene a theta en proababilidad,
que quiere decir?
"""

samples = list(range(1, 10000, 100))
expected_values = list(map(lambda e: 
                estimate_mean(gen_normal_sample(e)), samples))
plt.plot(samples, expected_values, 'o')
```

```{python}
MSG = """
Hacer un box plot con la distribución de probabilidad del estimador, la idea es
generar k muestras de tamano n, graficar la distribución de estas k muestras,
entre más grande el número k menos varianza debería tener la distribución.

El estimador es consistente, aun con muestras pequenas la distribución
del estimador tiende hacia el parametro.
"""
```

```{python}
df_e = estimators_samples_table(gen_normal_sample, estimate_mean)
fig = plt.figure(figsize=(16, 10))
ax = fig.add_subplot(1, 1, 1)
sns.boxplot(ax=ax, data=df_e, y="values", x="sample", hue="N", showfliers=False)
```

### Normal distribution: Error

```{python}
MSG = """
Lo que yo esperaria es que el mean square error del parametro disminuya conforme incrementa el número de muestras.
No siempre es el menor sin embargo el valor es más constante entre más muestras, no cambia tanto entre ejecuciones.
"""
```

```{python}
df_e = estimators_samples_table(gen_normal_sample, estimate_mean)
df_e.loc[:,"parameter"] = MU
df_e.loc[:,"square_error"] = (df_e["values"] - df_e["parameter"]).apply(lambda e: e**2)
df_e.head()
```

```{python}
df_e[["sample", "square_error"]].groupby(by="sample").describe()
```

### Uniform distribution

```{python}
LOW = 10
MAX = 1
MU = (MAX + LOW)/2

def gen_uniform_sample(N, lower=LOW, upper=MAX):
    return np.random.uniform(lower, upper, N)
```

### Uniform distribution: Sesgo

```{python}
MSG = """
El estimador es menos volatil cuando incrementa el número de muestras y no presenta ningún sesgo
"""

samples = list(range(1, 10000, 100))
expected_values = list(map(lambda e: check_bias(
    partial(gen_uniform_sample, 10), estimate_mean, N=e), samples))
plt.plot(samples, expected_values, 'o')
```

```{python}
MSG = """
Hacer un box plot con la distribución de probabilidad de la muestra con estos tamanos:
1, 10, 100, 1000, 10000, 100000.

El estimador no presenta ningún sesgo en ocasiones su valor es inferior y en otras superior al paramtero
"""

df = compute_samples_table(gen_uniform_sample, estimate_mean)
sns.boxplot(data=df, y="values", x="sample", showfliers=True)
```

### Uniform distribution: Consistencia

```{python}
MSG = """
El estimador mejora de forma considerable conforme incrementa el número de muestras, el parametro esta bn estimado
aunque la varianza incrementa cuando el tamaño de las muestras es muy pequeño.
"""
```

```{python}
df_e = estimators_samples_table(gen_uniform_sample, estimate_mean)
fig = plt.figure(figsize=(16, 10))
ax = fig.add_subplot(1, 1, 1)
sns.boxplot(ax=ax, data=df_e, y="values", x="sample", hue="N", showfliers=True)
```

### Uniform distribution: Error

```{python}
MSG = """
Lo que yo esperaria es que el mean square error del parametro disminuya conforme incrementa el número de muestras.
No siempre es el menor sin embargo el valor es más constante entre más muestras, no cambia tanto entre ejecuciones.
"""
```

```{python}
df_e = estimators_samples_table(gen_uniform_sample, estimate_mean)
df_e.loc[:,"parameter"] = MU
df_e.loc[:,"square_error"] = (df_e["values"] - df_e["parameter"]).apply(lambda e: e**2)
df_e.head()
```

```{python}
df_e[["sample", "square_error"]].groupby(by="sample").describe()
```

### Exponential distribution

```{python}
LAMBDA = 1.0
MU = 1/LAMBDA

def gen_exponential_sample(N, lbda=LAMBDA):
    return np.random.exponential(lbda, N)
```

### Exponential distribution: Sesgo

```{python}
MSG = """
El estimador es menos volatil cuando incrementa el número de muestras y no presenta ningún sesgo
"""

samples = list(range(1, 10000, 100))
expected_values = list(map(lambda e: check_bias(
    partial(gen_exponential_sample, 10), estimate_mean, N=e), samples))
plt.plot(samples, expected_values, 'o')
```

```{python}
MSG = """
Hacer un box plot con la distribución de probabilidad de la muestra con estos tamanos:
1, 10, 100, 1000, 10000, 100000.

Me da la impresión que el estimador tienen un sesgo positivo
"""

df = compute_samples_table(gen_exponential_sample, estimate_mean)
sns.boxplot(data=df, y="values", x="sample", showfliers=True)
```

### Exponential distribution: Consistencia

```{python}
MSG = """
El estimador no funciona bn con muestras de tamaño muy pequeño, aún cuando incrementa el número de muestras el valor
estimado del parametro se aleja del valor real, con muestras de tamaño 100 funciona bn aún cuando el número de muestras
es pequeño.
"""
```

```{python}
df_e = estimators_samples_table(gen_exponential_sample, estimate_mean)
fig = plt.figure(figsize=(16, 10))
ax = fig.add_subplot(1, 1, 1)
sns.boxplot(ax=ax, data=df_e, y="values", x="sample", hue="N", showfliers=True)
```

### Exponential distribution: Error

```{python}
MSG = """
Comparado con las distribución uniforme y normal el error es MUY grande, es como del 10000%
"""
```

```{python}
df_e = estimators_samples_table(gen_uniform_sample, estimate_mean)
df_e.loc[:,"parameter"] = MU
df_e.loc[:,"square_error"] = (df_e["values"] - df_e["parameter"]).apply(lambda e: e**2)
df_e.head()
```

```{python}
df_e[["sample", "square_error"]].groupby(by="sample").describe()
```

```{python}

```
